{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5455d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50954c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/22 14:49:24 WARN Utils: Your hostname, Heewonui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.11 instead (on interface en0)\n",
      "22/11/22 14:49:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/heewonkim/opt/anaconda3/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/22 14:49:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# 스키마를 정의 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7198b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 데이터 생성\n",
    "data = [[1,\"Jules\",\"Damji\",\"https:titit.co.kr\",\"1/4/2016\",4425,[\"twitter\",\"LinkedIn\"]],\n",
    "       [2,\"Kim\",\"Ddo\",\"https:titit1.co.kr\",\"1/5/2015\",4115,[\"twitter\",\"kind\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e86d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wh/5g5p83j54pg070vprb1sydyh0000gn/T/ipykernel_16244/4018585625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ex3_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             .getOrCreate())\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mblogs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mblogs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblogs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'schema' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"Ex3_1\")\n",
    "            .getOrCreate())\n",
    "    blogs_df = spark.createDataFrame(data, schema)\n",
    "    blogs_df.show()\n",
    "    print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c4c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6f4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_row = Row(2,4,\"$4\",\"W3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d89ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14ce0a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wh/5g5p83j54pg070vprb1sydyh0000gn/T/ipykernel_16244/247671100.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Matei za\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kim hee\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauthors_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"State\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauthors_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "rows = [Row(\"Matei za\",\"CA\"),Row(\"Kim hee\",\"CC\")]\n",
    "authors_df = spark.createDataFrame(rows,[\"name\",\"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e80e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfe833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d636413",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c01c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema = StructType()\n",
    "schema = \"'ID' INT, 'First' STRING, 'Last' STRING, 'Url' STRING, 'Published' STRING, 'Hits' INT,'Campaigns' ARRAY<STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e320ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = \"/Users/heewonkim/Desktop/python_data/business-price-indexes-september-2022-quarter-csv.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4344492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = spark.read.csv(read_file,header =True,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8e058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9f51878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| Period|count|\n",
      "+-------+-----+\n",
      "|2022.09|  778|\n",
      "|2022.06|  778|\n",
      "|2022.03|  778|\n",
      "|2021.12|  778|\n",
      "|2021.09|  778|\n",
      "|2021.06|  778|\n",
      "|2021.03|  778|\n",
      "|2020.12|  778|\n",
      "|2020.09|  778|\n",
      "|2020.06|  778|\n",
      "|2020.03|  778|\n",
      "|2019.12|  778|\n",
      "|2019.09|  778|\n",
      "|2019.06|  778|\n",
      "|2019.03|  778|\n",
      "|2018.12|  778|\n",
      "|2018.09|  778|\n",
      "|2018.06|  778|\n",
      "|2018.03|  778|\n",
      "|2017.12|  778|\n",
      "|2017.09|  778|\n",
      "|2017.06|  778|\n",
      "|2017.03|  778|\n",
      "|2016.12|  778|\n",
      "|2016.09|  778|\n",
      "|2016.06|  778|\n",
      "|2016.03|  778|\n",
      "|2015.12|  778|\n",
      "|2015.09|  778|\n",
      "|2015.06|  778|\n",
      "|2015.03|  778|\n",
      "|2014.12|  778|\n",
      "|2014.09|  778|\n",
      "|2014.06|  778|\n",
      "|2014.03|  778|\n",
      "|2013.12|  776|\n",
      "|2013.09|  700|\n",
      "|2013.06|  700|\n",
      "|2013.03|  700|\n",
      "|2012.12|  700|\n",
      "+-------+-----+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(file_df\n",
    " .select(\"Period\")\n",
    " .groupBy(\"Period\")\n",
    " .count()\n",
    " .orderBy(\"Period\", ascending=False) # 역순(desc)\n",
    " .show(n=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d2a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83109ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = Row(350,True,\"Learning Spark 2E\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d93ef608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16070d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
