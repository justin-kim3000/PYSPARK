{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb44d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24768b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76424c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa891bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb561ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"tess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc7e3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee4cebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf39c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a*a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47015928",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7081aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = pd.Series([1,2,3])\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed1681bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "907455f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=========================>                               (5 + 6) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e972a2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.13\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa26d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16172b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- First: string (nullable = false)\n",
      " |-- Last: string (nullable = false)\n",
      " |-- Url: string (nullable = false)\n",
      " |-- Published: string (nullable = false)\n",
      " |-- Hits: integer (nullable = false)\n",
      " |-- Campaigns: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n",
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n",
      "StructType(List(StructField(Id,IntegerType,false),StructField(First,StringType,false),StructField(Last,StringType,false),StructField(Url,StringType,false),StructField(Published,StringType,false),StructField(Hits,IntegerType,false),StructField(Campaigns,ArrayType(StringType,true),false)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# define schema for our data\n",
    "schema = StructType([\n",
    "   StructField(\"Id\", IntegerType(), False),\n",
    "   StructField(\"First\", StringType(), False),\n",
    "   StructField(\"Last\", StringType(), False),\n",
    "   StructField(\"Url\", StringType(), False),\n",
    "   StructField(\"Published\", StringType(), False),\n",
    "   StructField(\"Hits\", IntegerType(), False),\n",
    "   StructField(\"Campaigns\", ArrayType(StringType()), False)])\n",
    "\n",
    "#create our data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "      ]\n",
    "# main program\n",
    "if __name__ == \"__main__\":\n",
    "   #create a SparkSession\n",
    "   spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"Example-3_6\")\n",
    "       .getOrCreate())\n",
    "   # create a DataFrame using the schema defined above\n",
    "   blogs_df = spark.createDataFrame(data, schema)\n",
    "   # show the DataFrame; it should reflect our table above\n",
    "   blogs_df.show()\n",
    "   print()\n",
    "   # print the schema used by Spark to process the DataFrame\n",
    "   print(blogs_df.printSchema())\n",
    "   # Show columns and expressions\n",
    "   blogs_df.select(expr(\"Hits\") * 2).show(2)\n",
    "   blogs_df.select(col(\"Hits\") * 2).show(2)\n",
    "   blogs_df.select(expr(\"Hits * 2\")).show(2)\n",
    "   # show heavy hitters\n",
    "   blogs_df.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()\n",
    "   print(blogs_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "624d2086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|spark.sql.adaptiv...|<value of spark.s...|\n",
      "|spark.sql.adaptiv...|                true|\n",
      "|spark.sql.adaptiv...|         <undefined>|\n",
      "|spark.sql.adaptiv...|         <undefined>|\n",
      "|spark.sql.adaptiv...|               false|\n",
      "|spark.sql.adaptiv...|                true|\n",
      "|spark.sql.adaptiv...|         <undefined>|\n",
      "|spark.sql.adaptiv...|                true|\n",
      "|spark.sql.adaptiv...|                   5|\n",
      "|spark.sql.adaptiv...|               256MB|\n",
      "|spark.sql.ansi.en...|               false|\n",
      "|spark.sql.autoBro...|                10MB|\n",
      "|spark.sql.avro.co...|              snappy|\n",
      "|spark.sql.avro.de...|                  -1|\n",
      "|spark.sql.avro.fi...|                true|\n",
      "|spark.sql.broadca...|                 300|\n",
      "|spark.sql.bucketi...|               false|\n",
      "|spark.sql.bucketi...|                   4|\n",
      "|spark.sql.cache.s...|org.apache.spark....|\n",
      "|spark.sql.catalog...|         <undefined>|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET -v\").select(\"key\",\"value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2887934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a46071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 14:13:05 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d434f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cdfee6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wh/5g5p83j54pg070vprb1sydyh0000gn/T/ipykernel_21574/132930357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_mongo_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common_function'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from common_function import get_mongo_info\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19553fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"kafka_streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # 이거 없으면 INFO메시지가 너무 많이 뜬다. 궁금하면 주석처리하고 실행해보자.\n",
    "\n",
    "kafka_username, kafka_password = '', ''\n",
    "kafka_topic_name = ''\n",
    "kafka_bootstrap_server = ''\n",
    "options = {\n",
    "    \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{}\" password=\"{}\";'.format(kafka_username, kafka_password),\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_PLAINTEXT\",\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_server,\n",
    "    \"group.id\": 'test',\n",
    "    \"subscribe\": kafka_topic_name,\n",
    "}\n",
    "df = spark.readStream.format(\"kafka\").options(**options).load()\n",
    "\n",
    "def write_mongo_row(df, epoch_id):\n",
    "    if df.rdd.isEmpty():\n",
    "        return\n",
    "    # df.show()  # 테스트용\n",
    "\n",
    "    mongo_host, mongo_port, mongo_id, mongo_password, mongo_uri_format = get_mongo_info()\n",
    "    mongo_uri_format = mongo_uri_format.format(mongo_id, urllib.parse.quote(mongo_password), mongo_host, mongo_port,\n",
    "                                               'db.streaming_test')\n",
    "    # mongo_uri_format = \"mongodb://{몽고 id}:{몽고 pw}@{몽고 address}:{몽고 port}/{몽고 db name}.{몽고 collection name}\"  # uri포맷은 이와 같다.\n",
    "    df.write \\\n",
    "        .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"spark.mongodb.output.uri\", '{}?authSource=admin'.format(mongo_uri_format)) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "query = df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_mongo_row) \\\n",
    "    .start()\n",
    "\n",
    "# query = df \\\n",
    "#     .selectExpr(\"*\") \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()  # 콘솔로 출력하는 예제. kafka같은 스트리밍으로 들어오는 데이터의 outputMode를 complete로 하면 에러뜨면서 안됨.\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec58dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
